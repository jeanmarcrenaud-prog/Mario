name: Ollama CI

on:
  push:
    branches: [main]
  pull_request:

jobs:
  ollama-test:
    runs-on: self-hosted
    # Si vous avez plusieurs runners, vous pouvez cibler par label
    # labels: [windows, ollama]

    steps:
      - uses: actions/checkout@v4

      # 1️⃣ Démarrer Ollama
      - name: Start Ollama (Docker)
        shell: pwsh
        run: |
          docker run -d --name ollama `
            -p 11434:11434 `
            --gpus all `
            ollama/ollama
          # Attendre que le serveur soit prêt
          Start-Sleep -Seconds 10

      # 2️⃣ Vérifier que le serveur est en ligne
      - name: Ping Ollama
        shell: pwsh
        run: |
          $response = Invoke-RestMethod -Uri http://localhost:11434/api/ping
          if ($response.status -ne 'ok') { throw "Ollama not ready" }

      # 3️⃣ Pull du modèle
      - name: Pull model
        shell: pwsh
        run: |
          Invoke-RestMethod -Method Post `
            -Uri http://localhost:11434/api/pull `
            -Body '{"name":"llama2"}' `
            -ContentType 'application/json'

      # 4️⃣ Test de chat
      - name: Test chat
        shell: pwsh
        run: |
          $chat = Invoke-RestMethod -Method Post `
            -Uri http://localhost:11434/api/chat `
            -Body '{"model":"llama2","messages":[{"role":"user","content":"Bonjour"}]}' `
            -ContentType 'application/json'
          $chat | ConvertTo-Json -Depth 10 | Write-Host

      # 5️⃣ Nettoyage
      - name: Stop Ollama
        shell: pwsh
        run: docker stop ollama
