import requests
import json
from typing import List, Dict, Generator, Optional
from ..utils.logger import logger

class LLMAdapter:
    """Adaptateur pour diff√©rents mod√®les LLM."""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.current_model = "qwen3-coder:latest"  # Mod√®le par d√©faut
        self.is_available = self._check_connection()
        logger.info(f"LLMAdapter initialis√© - Base URL: {base_url}")
    
    def _check_connection(self) -> bool:
        """V√©rifie la connexion au serveur LLM."""
        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)
            if response.status_code == 200:
                logger.info("‚úÖ Connexion LLM √©tablie")
                return True
            else:
                logger.warning("‚ö†Ô∏è Serveur LLM non disponible")
                return False
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Impossible de se connecter au LLM: {e}")
            return False
    
    def set_model(self, model_name: str):
        """D√©finit le mod√®le √† utiliser."""
        self.current_model = model_name
        logger.info(f"Mod√®le LLM d√©fini: {model_name}")
    
    def get_available_models(self) -> List[str]:
        """Retourne la liste des mod√®les disponibles."""
        try:
            if not self.is_available:
                return []
            
            response = requests.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                models_data = response.json()
                models = [model["name"] for model in models_data.get("models", [])]
                logger.info(f"Mod√®les disponibles: {models}")
                return models
            return []
        except Exception as e:
            logger.error(f"Erreur r√©cup√©ration mod√®les: {e}")
            return []
    
    def chat(self, messages: List[Dict[str, str]], temperature: float = 0.7) -> str:
        """
        G√©n√®re une r√©ponse compl√®te.
        
        Args:
            messages: Liste des messages de conversation
            temperature: Temp√©rature du mod√®le (0.0-1.0)
            
        Returns:
            R√©ponse du mod√®le
        """
        try:
            if not self.is_available:
                return self._get_fallback_response(messages)
            
            payload = {
                "model": self.current_model,
                "messages": messages,
                "stream": False,
                "options": {
                    "temperature": temperature
                }
            }
            
            logger.info(f"ü§ñ Appel LLM avec {len(messages)} messages...")
            
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=payload,
                timeout=300  # 5 minutes timeout
            )
            
            if response.status_code == 200:
                result = response.json()
                answer = result.get("message", {}).get("content", "")
                logger.info(f"‚úÖ R√©ponse LLM: {answer[:100]}...")
                return answer
            else:
                logger.error(f"‚ùå Erreur LLM: {response.status_code} - {response.text}")
                return self._get_error_response()
                
        except Exception as e:
            logger.error(f"‚ùå Erreur appel LLM: {e}")
            return self._get_error_response()
    
    def chat_stream(self, messages: List[Dict[str, str]], temperature: float = 0.7) -> Generator[str, None, None]:
        """
        G√©n√®re une r√©ponse en streaming.
        
        Args:
            messages: Liste des messages de conversation
            temperature: Temp√©rature du mod√®le
            
        Yields:
            Morceaux de r√©ponse du mod√®le
        """
        try:
            if not self.is_available:
                yield from self._get_fallback_response_stream(messages)
                return
            
            payload = {
                "model": self.current_model,
                "messages": messages,
                "stream": True,
                "options": {
                    "temperature": temperature
                }
            }
            
            logger.info(f"ü§ñ Appel LLM streaming avec {len(messages)} messages...")
            
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=payload,
                timeout=300,
                stream=True
            )
            
            if response.status_code == 200:
                for line in response.iter_lines():
                    if line:
                        try:
                            data = json.loads(line.decode('utf-8'))
                            if 'message' in data and 'content' in data['message']:
                                content = data['message']['content']
                                if content:
                                    yield content
                            elif 'done' in data and data['done']:
                                break
                        except json.JSONDecodeError:
                            continue
            else:
                logger.error(f"‚ùå Erreur streaming LLM: {response.status_code}")
                yield "[ERREUR] Impossible de g√©n√©rer une r√©ponse"
                
        except Exception as e:
            logger.error(f"‚ùå Erreur streaming LLM: {e}")
            yield "[ERREUR] Erreur de connexion au mod√®le"
    
    def _get_fallback_response(self, messages: List[Dict[str, str]]) -> str:
        """R√©ponse de secours quand LLM non disponible."""
        last_message = messages[-1]["content"] if messages else ""
        return f"Je comprends votre message: '{last_message}'. C'est une r√©ponse simul√©e car le mod√®le n'est pas disponible."
    
    def _get_fallback_response_stream(self, messages: List[Dict[str, str]]) -> Generator[str, None, None]:
        """R√©ponse de secours en streaming."""
        last_message = messages[-1]["content"] if messages else ""
        response = f"Je comprends votre message: '{last_message}'. C'est une r√©ponse simul√©e car le mod√®le n'est pas disponible."
        for char in response:
            yield char
    
    def _get_error_response(self) -> str:
        """R√©ponse d'erreur."""
        return "[ERREUR] Impossible de g√©n√©rer une r√©ponse. Veuillez v√©rifier que le mod√®le est d√©marr√©."
    
    def test_connection(self) -> bool:
        """Teste la connexion au LLM."""
        try:
            test_messages = [{"role": "user", "content": "Bonjour"}]
            response = self.chat(test_messages)
            success = "Bonjour" in response or len(response) > 0
            if success:
                logger.info("‚úÖ Test connexion LLM r√©ussi")
            else:
                logger.error("‚ùå Test connexion LLM √©chou√©")
            return success
        except Exception as e:
            logger.error(f"‚ùå Test connexion LLM √©chou√©: {e}")
            return False
