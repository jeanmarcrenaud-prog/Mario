from typing import List, Dict, Generator, Optional
from ..adapters.llm_adapter import LLMAdapter
from ..utils.logger import logger

class LLMService:
    """Service de gestion des modÃ¨les LLM."""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.llm_adapter = LLMAdapter(base_url)
        self.current_model = "qwen3-coder:latest"
        logger.info("LLMService initialisÃ©")
    
    def set_model(self, model_name: str):
        """DÃ©finit le modÃ¨le Ã  utiliser."""
        self.current_model = model_name
        self.llm_adapter.set_model(model_name)
        logger.info(f"ModÃ¨le LLM dÃ©fini: {model_name}")
    
    def get_available_models(self) -> List[str]:
        """Retourne la liste des modÃ¨les disponibles."""
        try:
            return self.llm_adapter.get_available_models()
        except Exception as e:
            logger.error(f"Erreur rÃ©cupÃ©ration modÃ¨les: {e}")
            return ["qwen3-coder"]

    def _get_fallback_response(self, messages: List[Dict[str, str]]) -> str:
        """RÃ©ponse de secours quand LLM non disponible."""
        last_message = messages[-1]["content"] if messages else ""
        return f"Je comprends votre message: '{last_message}'. C'est une rÃ©ponse simulÃ©e car le modÃ¨le n'est pas disponible."
    
    def generate_response(self, messages: List[Dict[str, str]], temperature: float = 0.7) -> str:
        """
        GÃ©nÃ¨re une rÃ©ponse complÃ¨te.
        
        Args:
            messages: Liste des messages de conversation
            temperature: TempÃ©rature du modÃ¨le
            
        Returns:
            RÃ©ponse du modÃ¨le
        """
        try:
            if not messages:
                return "Aucun message Ã  traiter."
            
            # Ajouter le contexte si nÃ©cessaire
            enhanced_messages = self._enhance_messages(messages)
            
            logger.info(f"ğŸ¤– GÃ©nÃ©ration rÃ©ponse avec {len(enhanced_messages)} messages...")
            response = self.llm_adapter.chat(enhanced_messages, temperature)
            
            return response
            
        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©ration rÃ©ponse: {e}")
            return "[ERREUR] Impossible de gÃ©nÃ©rer une rÃ©ponse"
    
    def generate_response_stream(self, messages: List[Dict[str, str]], temperature: float = 0.7) -> Generator[str, None, None]:
        """
        GÃ©nÃ¨re une rÃ©ponse en streaming.
        
        Args:
            messages: Liste des messages de conversation
            temperature: TempÃ©rature du modÃ¨le
            
        Yields:
            Morceaux de rÃ©ponse du modÃ¨le
        """
        try:
            if not messages:
                yield "Aucun message Ã  traiter."
                return
            
            enhanced_messages = self._enhance_messages(messages)
            
            logger.info(f"ğŸ¤– GÃ©nÃ©ration rÃ©ponse streaming avec {len(enhanced_messages)} messages...")
            yield from self.llm_adapter.chat_stream(enhanced_messages, temperature)
            
        except Exception as e:
            logger.error(f"âŒ Erreur gÃ©nÃ©ration rÃ©ponse streaming: {e}")
            yield "[ERREUR] Impossible de gÃ©nÃ©rer une rÃ©ponse"
    
    def _enhance_messages(self, messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """AmÃ©liore les messages avec du contexte si nÃ©cessaire."""
        # Pour le moment, retourne les messages tels quels
        # Vous pouvez ajouter ici du contexte systÃ¨me, des instructions, etc.
        return messages
    
    def is_available(self) -> bool:
        """VÃ©rifie si le service LLM est disponible."""
        return self.llm_adapter.is_available
    
    def test_service(self) -> bool:
        """Teste le service LLM."""
        try:
            logger.info("ğŸ§ª Test du service LLM...")
            return self.llm_adapter.test_connection()
        except Exception as e:
            logger.error(f"âŒ Test service LLM Ã©chouÃ©: {e}")
            return False
